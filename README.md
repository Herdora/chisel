Keys & Caches 

![Keys & Caches Banner](assets/banner.png)

Welcome toÂ Keys & CachesÂ - the fastest way to run PyTorch models on cloud GPUs with automatic profiling and performance insights.
ğŸ“š Documentation Overview

This documentation will help you get started with Keys & Caches and make the most of its powerful features for GPU-accelerated machine learning.
Quick Navigation

* ğŸš€ Getting StartedÂ - Installation, setup, and your first GPU job
* ğŸ’¡ ExamplesÂ - Comprehensive examples and use cases
* ğŸ“ Contact & SupportÂ - Get help and connect with the community

What is Keys & Caches?

Keys & Caches is a command-line tool that makes it effortless to run PyTorch models on high-performance cloud GPUs. With just one command, you can:
* ğŸš€ Submit jobs to cloud GPUsÂ - Access A100, H100, and L4 GPUs instantly
* ğŸ“Š Get automatic profilingÂ - Detailed performance traces for every model forward pass
* ğŸ” Debug performance bottlenecksÂ - Chrome trace format for visual analysis
* âš¡ Stream real-time logsÂ - Watch your training progress live
* ğŸ’° Pay only for what you useÂ - No idle time charges
Key Features

ğŸ¯Â One-Command Deployment

# Run any PyTorch script on cloud GPUs
kandc python train.py --model-size large --epochs 100

ğŸ“ˆÂ Automatic Model Profiling

from kandc import capture_model_class

@capture_model_class(model_name="MyModel")
class MyModel(nn.Module):
    # Your model automatically gets profiled!

ğŸ®Â Flexible GPU Configurations

* A100 GPUsÂ (40GB/80GB) - Proven performance for training and inference
* H100 GPUsÂ (80GB) - Latest architecture with enhanced performance
* L4 GPUsÂ (24GB) - Cost-effective option for efficient workloads
* Scale 1-8 GPUsÂ - From development to massive scale training
ğŸ”§Â Simple Command Formats

# Two command formats:
kandc python script.py --script-args                                  # Interactive
chisel --app-name job --gpu H100:4 -- python script.py --script-args  # Separator


Who Should Use Keys & Caches?

ğŸ§‘â€ğŸ”¬Â Machine Learning Researchers

* Quickly test models on powerful GPUs without infrastructure setup
* Get detailed performance profiles to optimize model architectures
* Scale experiments from 1 to 8 GPUs seamlessly
ğŸ¢Â ML Engineers & Data Scientists

* Accelerate training jobs without managing cloud infrastructure
* Debug performance issues with automatic profiling
* Iterate faster with real-time progress monitoring
ğŸ“Â Students & Educators

* Email us atÂ founders@herdora.comÂ for free credits!
* Access high-end GPUs for coursework and research
* Learn about model optimization with built-in profiling tools
* Focus on ML concepts, not DevOps complexity
ğŸš€Â Startups & Small Teams

* Get enterprise-grade GPU access without upfront costs
* Scale compute resources based on actual needs
* Streamline ML workflows from development to production

Why Choose Keys & Caches?

ğŸš€ Instant Access

No account setup, no credit cards, no waiting. Install and run immediately.
ğŸ’¡ Built for ML

Purpose-built for PyTorch with automatic model profiling and optimization insights.
ğŸ¯ Developer-Friendly

Simple CLI that works with your existing code. No code changes required.
ğŸ“Š Performance-First

Every job includes detailed performance traces to help you optimize your models.
ğŸ’° Cost-Effective

Pay only for actual GPU time. No idle charges, no minimum commitments.

Ready to Get Started?

ğŸ‘‰Â Jump to Getting Started GuideÂ to install Keys & Caches and run your first GPU job in under 5 minutes!

Quick Example

Here's how easy it is to run a PyTorch model on cloud GPUs:
# your_model.py
import torch
import torch.nn as nn
from kandc import capture_model_class

@capture_model_class(model_name="SimpleModel")
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(784, 10)
    
    def forward(self, x):
        return self.linear(x)

# Train your model...
model = MyModel()
x = torch.randn(32, 784)
output = model(x)  # This gets automatically profiled!

# Run on cloud GPUs with one command
kandc python your_model.py

That's it! Your model runs on high-performance GPUs with automatic profiling. ğŸ‰

Ready to accelerate your ML workflows? Let's get started!
